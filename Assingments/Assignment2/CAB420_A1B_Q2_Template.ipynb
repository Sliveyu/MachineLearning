{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e285180",
   "metadata": {},
   "source": [
    "# CAB420 Assignment 1B Question 2: Template\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a helper function to load in the Oxford-IIIT Pets dataset suitable for classification and semantic segmentation, to help with Assignment 1B, Question 2.\n",
    "\n",
    "It also provides an example of how to load in the MobileNetV3Small Network which you are required to fine tune for the second part of the question.\n",
    "\n",
    "Please read the comments and instructions within this notebook. It has been carefully designed to help you with many of the tasks required.\n",
    "\n",
    "Please make sure you read the assignment brief on canvas, and check the FAQ for other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dde366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, SpatialDropout2D, Activation, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fce27",
   "metadata": {},
   "source": [
    "### Data loading and pre-processing functions\n",
    "\n",
    "We first provide some helper functions to format the data in the way we need. You shouldn't need to change these, though you are welcome to if you like.\n",
    "\n",
    "One thing you may want to do is create additional augmentation functions, and the ``flip_lr_augmentation`` function below could be used as a template to create additional augmentation types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404de120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_segmentation_mask(segmentation_mask):\n",
    "    \"\"\"preprocess the semgentation mask\n",
    "\n",
    "    The original segmentation mask has three categories.\n",
    "      foreground, background and outline\n",
    "    This function will just convert it to foreground and background \n",
    "\n",
    "    The original segmentation mask is also 1-index, so will convert it\n",
    "    to 0-index.\n",
    "\n",
    "    the original mask is represented as:\n",
    "    1 - edge of dog/cat and things like leashes etc.\n",
    "    2 - background\n",
    "    3 - foreground\n",
    "\n",
    "    we want to just keep the merge the edges and foreground of the doggo/catto, and\n",
    "    then treat it as a binary semantic segmentation task.\n",
    "    To achieve this, we will just subtract two, converting to values of [-1, 0, 1],\n",
    "    and then apply the abs function to convert the -1 values (edges) to the foreground.\n",
    "\n",
    "    Will also convert it to 32 bit float which will be needed for working with tf.\n",
    "    \n",
    "    Why am I doing it this way?\n",
    "     A reasonable question. Initially I tried to do it with just normal array indexing,\n",
    "     but this is a bit more work since the mask is a tensorflow tensor and not a np array.\n",
    "     We could alternatively convert it to an array, perform indexing and then map it back,\n",
    "     but this would have a performance overhead, which wouldn't be a big deal, but still.\n",
    "     With all that being said, I am doing it for you, so you don't have to.\n",
    "\n",
    "    Args:\n",
    "      segmentation_mask (array):\n",
    "        original segmentation mask\n",
    "\n",
    "    Returns:\n",
    "      preprocessed segmentation_mask\n",
    "    \"\"\"\n",
    "    return tf.abs(tf.cast(segmentation_mask, tf.float32) - 2)\n",
    "\n",
    "def return_image_label_mask(ds_out):\n",
    "    \"\"\" function to return image, class label and segmentation mask\n",
    "\n",
    "    The original dataset contains additional information, such as the filename and\n",
    "    the species. We don't care about any of that for this work, so will\n",
    "    discard them and just keep the original image as our input, and then\n",
    "    a tuple of our outputs that will be the class label and the semantic\n",
    "    segmentation mask.\n",
    "\n",
    "    Whilst we are here, we will also preprocess the segmentation mask.\n",
    "\n",
    "    Args:\n",
    "      ds_out: dict\n",
    "        original dataset output\n",
    "\n",
    "    Returns:\n",
    "       RGB image\n",
    "       tuple of class label and preprocessed segmentation mask\n",
    "    \"\"\"\n",
    "    # preprocess the segmentation mask\n",
    "    seg_mask =  preprocess_segmentation_mask(ds_out['segmentation_mask'])\n",
    "    image = tf.cast(ds_out['image'], tf.float32)\n",
    "    # image = standardise_image(image)\n",
    "    return image, (ds_out['label'], seg_mask)\n",
    "\n",
    "def mobilenet_preprocess_image(image):\n",
    "    \"\"\"Apply preprocessing that is suitable for MobileNetV3.\n",
    "    \n",
    "    Simply scales to ranges [-1, 1]\n",
    "    \n",
    "    \n",
    "    you should use this preprocessing for both your model and the mobilenet model\n",
    "    \"\"\"\n",
    "    image = (image - 127.5) / 255.0\n",
    "    return image\n",
    "        \n",
    "def unprocess_image(image):\n",
    "    \"\"\" undo preprocessing above so can plot images\"\"\"\n",
    "    image = image * 255.0 + 127.5\n",
    "    return image\n",
    "\n",
    "def preprocess_and_resize(image, output, image_size):\n",
    "    \"\"\"apply preprocessing steps above to images and resize images and maps\n",
    "    \n",
    "    Each image in the dataset is of a different size. The resizing will make sure\n",
    "    each image is the same size.\n",
    "    \"\"\"\n",
    "    # resize the image and the semantic segmentation mask\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "    image = mobilenet_preprocess_image(image)\n",
    "    mask = tf.image.resize(output[1], [image_size, image_size])\n",
    "    return image, (output[0], mask)\n",
    "\n",
    "def flip_lr_augmentation(image, output, flip_lr_prob):\n",
    "    \"\"\" function to return perform left-right flip augmentation\n",
    "\n",
    "    The function will flip the image along the left-right axis with\n",
    "    a defined probability.\n",
    "    \"\"\"\n",
    "\n",
    "    # randomly sample a value between 0 and 1\n",
    "    uniform_sample = tf.random.uniform([], minval=0, maxval=1)\n",
    "    # perform flip_lr with probability given by flip_lr_prob \n",
    "    flip_lr_cond = tf.math.less(uniform_sample, flip_lr_prob)\n",
    "    # output is a tuple of (class, segmentation_mask), pull out the segmentation mask\n",
    "    seg = output[1]\n",
    "    \n",
    "    # wrapper fn for when we do the flip\n",
    "    def flip():\n",
    "        flipped_image = tf.image.flip_left_right(image)\n",
    "        flipped_seg = tf.image.flip_left_right(seg)\n",
    "        return flipped_image, flipped_seg\n",
    "\n",
    "    # wrapper fn for when we do NOT flip\n",
    "    def no_flip():\n",
    "        return image, seg\n",
    "\n",
    "    # apply augmentation    \n",
    "    image, seg = tf.cond(flip_lr_cond, flip, no_flip)\n",
    "    # return the image, and output\n",
    "    return image, (output[0], seg)\n",
    "\n",
    "def select_tasks(image, output, classification=True, segmentation=True):\n",
    "    \"\"\"select the tasks to include the data\n",
    "\n",
    "    By default for each input there are two outputs. This function allows\n",
    "    you to select which outputs to use, so the problem can be reduced to a\n",
    "    single task problem for initial experimenting.    \n",
    "    \"\"\"\n",
    "    # both tasks\n",
    "    if classification and segmentation:\n",
    "        return image, output\n",
    "    # just classification\n",
    "    elif classification:\n",
    "        return image, output[0]\n",
    "    # just segmentation\n",
    "    elif segmentation:\n",
    "        return image, output[1]\n",
    "    # neither task, doesn't really make sense, so return the image\n",
    "    # for a self-supervised task\n",
    "    else:\n",
    "        return image, image\n",
    "\n",
    "class TrainForTime(keras.callbacks.Callback):\n",
    "    \"\"\"callback to terminate training after a time limit is reached\n",
    "\n",
    "    Can be used to control how long training runs for, and will terminate\n",
    "    training once a specified time limit is reached.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_time_mins=15,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_time_mins = train_time_mins\n",
    "        self.epochs = 0\n",
    "        self.train_time = 0\n",
    "        self.end_early = False\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # save the start time\n",
    "        self.start_time = tf.timestamp()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epochs += 1\n",
    "        current_time = tf.timestamp()\n",
    "        training_time = (current_time - self.start_time)\n",
    "        if (training_time / 60) > self.train_time_mins:\n",
    "            self.train_time = current_time - self.start_time\n",
    "            self.model.stop_training = True\n",
    "            self.end_early = True\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.end_early:\n",
    "            print('training time exceeded and ending early')\n",
    "            print(f'training ended on epoch {self.epochs}')\n",
    "            print(f'training time = {self.train_time / 60} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab427d50",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "We will now put the above functions together into a data loader that we can use to feed directly to the network. You can you this directly as it is. However, you may modify it to add some additional functionality such as further data augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b9d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oxford_pets(split,\n",
    "                     batch_size=233,\n",
    "                     classification=True,\n",
    "                     segmentation=True,\n",
    "                     shuffle=True,\n",
    "                     augment=True,\n",
    "                     image_size=300):\n",
    "    \"\"\"Load Oxford pets dataset for Assignment 1B\n",
    "\n",
    "    Function handles loading of data for 1b, included processing of images and\n",
    "    semantic segmentation masks. This function will\n",
    "    organise the tensorflow dataset to return an output that is a tuple, where\n",
    "    the tuple will be (classification_labels, segmentation_masks).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    split : string\n",
    "        either train or test string\n",
    "    classification : bool\n",
    "        whether to include classification labels\n",
    "    segmentation : bool\n",
    "        whether to include semantic segmentation masks\n",
    "    batch_size : int\n",
    "        size of batches to use\n",
    "    shuffle : bool\n",
    "        whether to shuffle the dataset (WILL ONLY APPLY TO TRAIN)\n",
    "    augment : bool\n",
    "        whether to augment the dataset (WILL ONLY APPLY TO TRAIN)\n",
    "    image_size : int\n",
    "        new image size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "       tf.Dataset containing the Oxford pets dataset\n",
    "    \"\"\"\n",
    "    # lets do some error checking first\n",
    "    # Check fior a valid dataset split, this must be train or test\n",
    "    if (split != 'train') and (split != 'test'):\n",
    "        raise ValueError('Arg for split must be either \\'train\\' or \\'test\\'')\n",
    "    if (not classification) and (not segmentation):\n",
    "        print(\"WARNING: One of the tasks (classification and segmentation) must be selected\")\n",
    "        print(\"Setting both to enabled\")\n",
    "        classification = True\n",
    "        segmentation = True\n",
    "    # check that if using the test split, shuffle if false. If not, print a warning and force shuffle to be false\n",
    "    if (split == 'test') and shuffle:\n",
    "        print(\"WARNING: shuffle is set to true, but have specified split to be \\'test\\'\")\n",
    "        print('The shuffle argument will be ignored')\n",
    "        shuffle = False\n",
    "    # check that if using the test split, augment if false. If not, print a warning and force augment to be false\n",
    "    if (split == 'test') and augment:\n",
    "        print(\"WARNING: augment is set to true, but have specified split to be \\'test\\'\")\n",
    "        print('The augment argument will be ignored')\n",
    "        augment = False\n",
    "   \n",
    "    # now start loading the dataset\n",
    "    ds = tfds.load('oxford_iiit_pet',\n",
    "                   split=split,\n",
    "                   with_info=False)\n",
    "    \n",
    "    # remove unnecessary dataset info\n",
    "    ds = ds.map(return_image_label_mask)\n",
    "\n",
    "    # augmentation\n",
    "    # only apply if in the training split and augment has been set to True\n",
    "    if split == 'train' and augment:\n",
    "        # apply a left-right flip with 50% probability\n",
    "        flip_lr_prob = 0.5\n",
    "        # flip operation\n",
    "        ds = ds.map(lambda inp, out: flip_lr_augmentation(inp, out, flip_lr_prob), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        # more augmentation operations could go here .....\n",
    "    \n",
    "    # Final processing of the data \n",
    "    # here we will resize the data, and add the preprocessing that is needed for compatability with the mobilenet models.\n",
    "    ds = ds.map(lambda inp, out: preprocess_and_resize(inp, out, image_size))\n",
    "\n",
    "    # and now remove any tasks that we don't want. Note that we call this last as it means that all the other functions\n",
    "    # can safely assume that data for both tasks is in the dataset\n",
    "    ds = ds.map(lambda inp, out: select_tasks(inp, out, classification, segmentation))\n",
    "\n",
    "    # if in the training split, and shuffle is true, shuffle the data\n",
    "    if split == 'train' and shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # return the loaded and processed dataset\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811718fc",
   "metadata": {},
   "source": [
    "### Testing the provided data loader.\n",
    "\n",
    "We'll now test the data loader and plot some examples to confirm it's working. **NOTE: some poor defaults are specified below for image size and batch size. Set these to something more appropriate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25392c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\livin\\tensorflow_datasets\\oxford_iiit_pet\\3.2.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a33bcba1397431582e68324eb1e6484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03512cc91a7a41769a97a185ff3a64e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73218da1e7644cb8ced6405981b817c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ExtractError",
     "evalue": "Error while extracting C:\\Users\\livin\\tensorflow_datasets\\downloads\\robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz to C:\\Users\\livin\\tensorflow_datasets\\downloads\\extracted\\TAR_GZ.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz: Failed to create a NewWriteableFile: C:\\Users\\livin\\tensorflow_datasets\\downloads\\extracted\\TAR_GZ.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz.incomplete_f9f6130b53ab4ef8a9b7196bbf9b6b76\\images\\american_pit_bull_terrier_67.jpg : There is not enough space on the disk.\r\n; operation in progress",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py:111\u001b[0m, in \u001b[0;36m_Extractor._extract\u001b[1;34m(self, from_path, method, to_path)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[1;32m--> 111\u001b[0m   \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pbar_path\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py:142\u001b[0m, in \u001b[0;36m_copy\u001b[1;34m(src_data, dest_path)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dest_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dest_file:\n\u001b[1;32m--> 142\u001b[0m   \u001b[43mdest_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:100\u001b[0m, in \u001b[0;36mFileIO.write\u001b[1;34m(self, file_content)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prewrite_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writable_file\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    102\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_bytes(file_content, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__encoding))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:85\u001b[0m, in \u001b[0;36mFileIO._prewrite_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for writing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writable_file \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWritableFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to create a NewWriteableFile: C:\\Users\\livin\\tensorflow_datasets\\downloads\\extracted\\TAR_GZ.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz.incomplete_f9f6130b53ab4ef8a9b7196bbf9b6b76\\images\\american_pit_bull_terrier_67.jpg : There is not enough space on the disk.\r\n; operation in progress",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExtractError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m273\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# load training data, note that shuffle and augment are true\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train_class_seg \u001b[38;5;241m=\u001b[39m \u001b[43mload_oxford_pets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# load testing data, note that shuffle and augment are false (though if they weren't, the data loader would force these to be false)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m test_class_seg \u001b[38;5;241m=\u001b[39m load_oxford_pets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, classification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, segmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, image_size\u001b[38;5;241m=\u001b[39mimage_size)\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mload_oxford_pets\u001b[1;34m(split, batch_size, classification, segmentation, shuffle, augment, image_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m     augment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# now start loading the dataset\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moxford_iiit_pet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m               \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m               \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# remove unnecessary dataset info\u001b[39;00m\n\u001b[0;32m     62\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(return_image_label_mask)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:649\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    643\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[0;32m    644\u001b[0m     name,\n\u001b[0;32m    645\u001b[0m     data_dir,\n\u001b[0;32m    646\u001b[0m     builder_kwargs,\n\u001b[0;32m    647\u001b[0m     try_gcs,\n\u001b[0;32m    648\u001b[0m )\n\u001b[1;32m--> 649\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:508\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    507\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 508\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:691\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 691\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    697\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    698\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    699\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1547\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1546\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1547\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[0;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[0;32m   1554\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[0;32m   1555\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[0;32m   1556\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[0;32m   1557\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\datasets\\oxford_iiit_pet\\oxford_iiit_pet_dataset_builder.py:92\u001b[0m, in \u001b[0;36mBuilder._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns splits.\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Download images and annotations that come in separate archives.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Note, that the extension of archives is .tar.gz even though the actual\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# archives format is uncompressed tar.\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m dl_paths \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_BASE_URL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/images.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_BASE_URL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/annotations.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m images_path_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dl_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m annotations_path_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dl_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:688\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m    687\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[1;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m--> 831\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:832\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m    831\u001b[0m res \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m--> 832\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises\n\u001b[0;32m    833\u001b[0m )  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\promise\\promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\promise\\promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\promise\\promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[0;32m    225\u001b[0m     raise_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n\u001b[1;32m--> 226\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\promise\\promise.py:844\u001b[0m, in \u001b[0;36m_process_future_result.<locals>.handle_future_result\u001b[1;34m(future)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_future_result\u001b[39m(future):\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;66;03m# type: (Any) -> None\u001b[39;00m\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 844\u001b[0m         resolve(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    846\u001b[0m         tb \u001b[38;5;241m=\u001b[39m exc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py:122\u001b[0m, in \u001b[0;36m_Extractor._extract\u001b[1;34m(self, from_path, method, to_path)\u001b[0m\n\u001b[0;32m    116\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m max_length_dst_path \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m250\u001b[39m:\n\u001b[0;32m    117\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOn windows, path lengths greater than 260 characters may result\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in an error. See the doc to remove the limitation: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://docs.python.org/3/using/windows.html#removing-the-max-path-limitation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ExtractError(msg)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# `tf.io.gfile.Rename(overwrite=True)` doesn't work for non empty\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# directories, so delete destination first, if it already exists.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m to_path \u001b[38;5;241m=\u001b[39m epath\u001b[38;5;241m.\u001b[39mPath(to_path)\n",
      "\u001b[1;31mExtractError\u001b[0m: Error while extracting C:\\Users\\livin\\tensorflow_datasets\\downloads\\robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz to C:\\Users\\livin\\tensorflow_datasets\\downloads\\extracted\\TAR_GZ.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz: Failed to create a NewWriteableFile: C:\\Users\\livin\\tensorflow_datasets\\downloads\\extracted\\TAR_GZ.robots.ox.ac.uk_vgg_pets_imagesZxlcXhwB8atfm2pdIrjCelgNiW7ORYkX5h1Fkzf6MY0.tar.gz.incomplete_f9f6130b53ab4ef8a9b7196bbf9b6b76\\images\\american_pit_bull_terrier_67.jpg : There is not enough space on the disk.\r\n; operation in progress"
     ]
    }
   ],
   "source": [
    "# testing the data loader and plotting some images.\n",
    "# NOTE: the image size set here is all but definitely too large. You will need\n",
    "# to chage this yourself to something that is suitable given your constraints\n",
    "# NOTE: The batch size is also too large. This done on purpose force you to \n",
    "# pick a suitable batch size yourself\n",
    "image_size = 300\n",
    "batch_size = 273\n",
    "# load training data, note that shuffle and augment are true\n",
    "train_class_seg = load_oxford_pets('train', classification=True, segmentation=True, shuffle=True, augment=True, batch_size=batch_size, image_size=image_size)\n",
    "# load testing data, note that shuffle and augment are false (though if they weren't, the data loader would force these to be false)\n",
    "test_class_seg = load_oxford_pets('test', classification=True, segmentation=True, shuffle=False, augment=False, batch_size=batch_size, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3189609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_class_seg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# each sample of our dataset will be of the format\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# image, outputs\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# where outputs[0] = label\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# lets get a single batch, and plot just a few of them\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, output \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_class_seg\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mas_numpy_iterator(): \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_plot): \n\u001b[0;32m     15\u001b[0m         im \u001b[38;5;241m=\u001b[39m axs[\u001b[38;5;241m0\u001b[39m, i]\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39msqueeze(unprocess_image(image[i, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_class_seg' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets plot a few now to see some good kittens/doggos\n",
    "fig, axs = plt.subplots(2, 3, figsize=(8, 4), layout=\"constrained\")\n",
    "num_plot = 32\n",
    "i = 0\n",
    "\n",
    "# each sample of our dataset will be of the format\n",
    "# image, outputs\n",
    "# where outputs[0] = label\n",
    "#       outputs[1] = segmentation mask\n",
    "#\n",
    "#\n",
    "# lets get a single batch, and plot just a few of them\n",
    "for image, output in train_class_seg.take(1).as_numpy_iterator(): \n",
    "    for i in range(num_plot): \n",
    "        im = axs[0, i].imshow(np.squeeze(unprocess_image(image[i, ...])) / 255.0)\n",
    "        axs[0, i].set_title(output[0][i])\n",
    "        axs[0, i].axis('off')\n",
    "        im = axs[1, i].imshow(np.squeeze(output[1][i, ...]))\n",
    "        axs[1, i].axis('off')\n",
    "        \n",
    "        print(output[1].shape)\n",
    "        i += 1\n",
    "        if i >= num_plot:\n",
    "            break\n",
    "        \n",
    "plt.savefig('doggos_cattos.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a1fce",
   "metadata": {},
   "source": [
    "The images are showing correctly.\n",
    "\n",
    "**NOTE: You can ignore the JPEG wearning.**\n",
    "\n",
    "We can use the ``classification`` and ``segmentation`` flags to pull out just one output as well, as the below demonstrates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8426029-0281-4cea-91f4-3db67f636486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "# classification only; classification = True, segmentation = False (note batch size is 1 here)\n",
    "train_class_only = load_oxford_pets('train', classification=True, segmentation=False, shuffle=True, augment=True, batch_size=1, image_size=image_size)\n",
    "# segmentation only; classification = False, segmentation = True (note batch size is 1 here)\n",
    "train_seg_only = load_oxford_pets('train', classification=False, segmentation=True, shuffle=True, augment=True, batch_size=1, image_size=image_size)\n",
    "\n",
    "# test the classification only dataset\n",
    "# pull out one element\n",
    "inp, out = next(iter(train_class_only))\n",
    "# print the output\n",
    "print(out.numpy())\n",
    "\n",
    "# test the segmentation only dataset\n",
    "# pull out one element\n",
    "inp, out = next(iter(train_seg_only))\n",
    "# print just the output shape for the segmentation output \n",
    "print(out.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832b5d5",
   "metadata": {},
   "source": [
    "While for the question you do need to train networks to do both tasks simultaenously, when you starting playing with the problem it might be easier to get things working for one task, and then add the second.\n",
    "\n",
    "### Loading MobileNetV3Small base for fine tuning\n",
    "\n",
    "This model can be loaded directly from keras. By default, the model we download will be pre-trained on Imagenet dataset.\n",
    "\n",
    "Note that we will need to set the preprocessing option when loading this base network to False. This is because the `include_preprocessing` step is implemented in the Datasets we defined above.\n",
    "\n",
    "We also set `include_top=False`, to avoid loading our model with the final Dense classification layer which is used for the original Imagenet model.\n",
    "\n",
    "More details are available in the keras documentation [here](https://keras.io/api/applications/mobilenet/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5558bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "mobile_base = keras.applications.MobileNetV3Small(input_shape=(image_size, image_size, 3),\n",
    "                                                  include_top=False,\n",
    "                                                  include_preprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38b199-a2cf-4758-a38b-2f249d92c99c",
   "metadata": {},
   "source": [
    "For this task, can ignore the input_shape warning, though it is important to keep in mind the difference in size of data used for the pre-trained model and our data may have an impact on our model (what that impact might be is for you to investigate :) ). Depending on what input shape you select you may also be able to eliminate this.\n",
    "\n",
    "For more information on fine-tuning models, can refer to many of the examples from class, or the [Keras documentation](https://keras.io/guides/transfer_learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b07f0f-d05e-44bf-a3cf-54eb4746ca95",
   "metadata": {},
   "source": [
    "## Question 2 Template\n",
    "\n",
    "The following provides a starting point for your solution, and some suggestions on how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe4194d-48ea-4906-8e83-ede4a05cb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Data loading\n",
    "#\n",
    "# Use the provided data loader, setting your batch size and image size appropriately.\n",
    "# Note that you may want to add more augmentation into the data loader.\n",
    "# For initial experiments you may also want to turn one of classification or segmentation off to allow you to play with a single task.\n",
    "image_size = 300 # WARNING: THIS VALUE IS STUPID\n",
    "batch_size = 273 # WARNING: THIS VALUE IS STUPID\n",
    "# load training data, note that shuffle and augment are true\n",
    "train_class_seg = load_oxford_pets('train', classification=True, segmentation=True, shuffle=True, augment=True, batch_size=batch_size, image_size=image_size)\n",
    "# load testing data, note that shuffle and augment are false (though if they weren't, the data loader would force these to be false)\n",
    "test_class_seg = load_oxford_pets('test', classification=True, segmentation=True, shuffle=False, augment=False, batch_size=batch_size, image_size=image_size)\n",
    "\n",
    "# Note that you may also want a non-shuffled version of the training set to help you evaluate training performance. You could simply create another\n",
    "# training set object with the same settings, but shuffle set to False, i.e.\n",
    "# train_class_seg_ns = load_oxford_pets('train', classification=True, segmentation=True, shuffle=False, augment=True, batch_size=batch_size, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1a9edb5-3c24-4925-9bd5-1a6e6198d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# From-scratch DCNN\n",
    "#\n",
    "# Put your from-scratch DCNN here. This network will have one input (the input image), and two outputs (the class, and the segmentation map). You can\n",
    "# think of your model as having three components:\n",
    "#  - A backbone, which is going to take your input and encode that. This will possibly be collection of (probably mostly) 2D convolution layers \n",
    "#    and max pooling layers (and for which you may be able to find something fairly fit for purpose in lecture or prac examples).\n",
    "#  - A classification head, which will take the backbone output and via one or more dense layers do the classification task. This will likely only\n",
    "#    be a few layers at most.\n",
    "#  - A decoder head, which will do the semantic segmentation. This will look a lot like the back half of an autoencoder.\n",
    "#\n",
    "# As usual, the network does not need to be overly complex, but you will need to briefly explain your selection of this model, and you should avoid \n",
    "# networks so simple that they perform very badly.\n",
    "#\n",
    "# Take note of the order that the outputs appear in the data loader. The data loader will give you (classification_output, segmentation_output), so\n",
    "# your network should have the outputs in the same order.\n",
    "#\n",
    "# When first developing your model, you may want to start by getting a single output of the network going, and then adding the second output. For\n",
    "# example, you may build a network with the image input and the just the classification head, get that running, and then add the decoder head. You can\n",
    "# use the classification and segmentation flags in the data loader to turn off one of the outputs.\n",
    "\n",
    "# For model training, we have provided a callback, TrainForTime, which will terminate training after some time limit is reached. You could use this\n",
    "# as follows:\n",
    "#   train_time = 15 # 15-minute training limit\n",
    "#   my_awesome_model.fit(train_class_seg, epochs=epochs, verbose=True, validation_data=test_class_seg, callbacks=[TrainForTime(train_time)])\n",
    "# You don't have to use this, but you may wish to use this to ensure training doesn't take too long.\n",
    "# Note that if you don't train until convergence, you will need to explain what you observe with training and what the implications of this are.\n",
    "\n",
    "def build_from_scratch_model(input_shape):\n",
    "    \"\"\"Build a custom multi-task model from scratch for classification and segmentation.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Backbone\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Classification head\n",
    "    classification_flatten = layers.Flatten()(x)\n",
    "    classification_output = layers.Dense(37, activation='softmax', name='classification')(classification_flatten)\n",
    "\n",
    "    # Segmentation head\n",
    "    # Using a simple upscaling and convolution to produce segmentation masks\n",
    "    segmentation_upsample = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), activation='relu')(x)\n",
    "    segmentation_upsample = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), activation='relu')(segmentation_upsample)\n",
    "    segmentation_upsample = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), activation='relu')(segmentation_upsample)\n",
    "    segmentation_output = layers.Conv2D(1, (1, 1), activation='sigmoid', name='segmentation')(segmentation_upsample)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=[classification_output, segmentation_output])\n",
    "    #model = models.Model(inputs=inputs, outputs=segmentation_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the overall training and validation loss.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss for Custom Model')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Build model\n",
    "model_from_scratch = build_from_scratch_model((image_size, image_size, 3))\n",
    "\n",
    "# Compile models\n",
    "model_from_scratch.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'classification': 'sparse_categorical_crossentropy', 'segmentation': 'binary_crossentropy'},\n",
    "    metrics={'classification': 'accuracy', 'segmentation': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Callback to stop training based on time\n",
    "train_time_callback = TrainForTime(train_time_mins=15)\n",
    "\n",
    "# Use early stopping to halt training when validation loss stops improving\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the models\n",
    "history_from_scratch = model_from_scratch.fit(\n",
    "    train_ds,\n",
    "    epochs=20,  # You may adjust this based on the callback or your observations\n",
    "    validation_data=test_ds,\n",
    "    callbacks=[train_time_callback, early_stopping]\n",
    ")\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_from_scratch)\n",
    "\n",
    "print(\"Training loss:\", history_from_scratch.history['loss'])\n",
    "print(\"Validation loss:\", history_from_scratch.history['val_loss'])\n",
    "\n",
    "# Additional evaluation and visualization functions as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd9b5f6-487e-4d3f-9e1d-0d55c2b738b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Fine-tuned MobileNet\n",
    "#\n",
    "# Pur your mobilenet model here. This will have the same structure as the from-scratch network above, but now the backbone will be mobilenet rather\n",
    "# than something of your own design. The classification and segmentation heads will possibly be quite similar to what you have above. As per the \n",
    "# from-scratch network, you may want to build this with just one output to start with, and then add the second when you get that working.\n",
    "\n",
    "# load mobilenet backbone\n",
    "mobile_base = keras.applications.MobileNetV3Small(input_shape=(image_size, image_size, 3),\n",
    "                                                  include_top=False,# Do not include the classification head of the model\n",
    "                                                  include_preprocessing=False)\n",
    "\n",
    "# train the model. As per the from-scratch model, you may wish to use the callback - though if the model is not trained until convergence you will\n",
    "# need to explain the implications of this.\n",
    "def build_finetuned_model(input_shape):\n",
    "    # Load MobileNetV3Small as the backbone without the top layer\n",
    "    base_model = mobile_base   \n",
    "\n",
    "    # Get the output from the 'conv_1' layer\n",
    "    output_layer = base_model.get_layer('conv_1').output\n",
    "\n",
    "    # New classification head\n",
    "    classification_layer = layers.GlobalAveragePooling2D()(output_layer)\n",
    "    classification_output = layers.Dense(37, activation='softmax', name='classification')(classification_layer)\n",
    "\n",
    "    # Adding a segmentation decoder that upsamples the feature maps\n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), activation='relu')(output_layer)\n",
    "    x = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    segmentation_output = layers.Conv2D(1, (1, 1), activation='sigmoid', name='segmentation')(x)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Model(inputs=base_model.input, outputs=[classification_output, segmentation_output])\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the overall training and validation loss.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss for Fine-Tuned Model')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Build and compile the model\n",
    "model_finetuned = build_finetuned_model((image_size, image_size, 3))\n",
    "model_finetuned.compile(optimizer='adam',loss={'classification': 'sparse_categorical_crossentropy','segmentation': 'binary_crossentropy'},\n",
    "    metrics={'classification': 'accuracy','segmentation': 'accuracy'})\n",
    "\n",
    "\n",
    "# Callback to stop training based on time\n",
    "train_time_callback = TrainForTime(train_time_mins=15)\n",
    "# Use early stopping to halt training when validation loss stops improving\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the models\n",
    "history_finetuned = model_finetuned.fit(\n",
    "    train_ds,\n",
    "    epochs=20,  # You may adjust this based on the callback or your observations\n",
    "    validation_data=test_ds,\n",
    "    callbacks=[train_time_callback, early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_finetuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94533dc7-aef5-4164-b0a7-b3be42ad5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import sklearn\n",
    "\n",
    "# # from sklearn import metrics\n",
    "# # #\n",
    "# # # Evaluation\n",
    "# # #\n",
    "# # # You should evaluate both your networks for both tasks. To help generate metrics and plots, you may want to pull out the labels from the dataset\n",
    "# # # which you could do with something like the following:\n",
    "# # #\n",
    "# # y_class_test = np.concatenate([y[0] for x,y in test_ds.as_numpy_iterator()])\n",
    "# # y_seg_test = np.concatenate([y[1] for x,y in test_ds.as_numpy_iterator()])\n",
    "# # # \n",
    "# # # For evaluating the two tasks, the classification task is just like all the other classification tasks you've done this semseter and you can follow\n",
    "# # # how these have been dealt with. For the segmentation task, one way to look at this is as a binary classification task as you're classifiying each \n",
    "# # # pixel into foreground and background. With that in mind you could do something like this:\n",
    "# # #\n",
    "# # class_preds, seg_preds = model_from_scratch.predict(test_ds)\n",
    "# # seg_preds_flat = seg_preds.reshape(-1) > 0.5 # flatten predictions, turn into boolean vector where true is fgnd and false is bkgnd\n",
    "# # seg_gt_flat = y_seg_test.reshape(-1) > 0.5 # same as above, but for the ground truth, need to have both as a boolean vector\n",
    "# # cm = sklearn.metrics.confusion_matrix(seg_gt_flat, seg_preds_flat)\n",
    "# # #\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Retrieve the ground truth labels for tasks\n",
    "y_class_test = np.concatenate([y[0] for x, y in test_ds.as_numpy_iterator()])\n",
    "y_seg_test = np.concatenate([y[1] for x, y in test_ds.as_numpy_iterator()])\n",
    "\n",
    "############ FROM SCRATH MODEL #################\n",
    "# Get predictions from the model\n",
    "class_preds, seg_preds = model_from_scratch.predict(test_ds)\n",
    "\n",
    "# Process classification results\n",
    "class_preds_labels = np.argmax(class_preds, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Process segmentation results\n",
    "seg_preds_flat = seg_preds.reshape(-1) > 0.5  # Flatten and threshold predictions\n",
    "seg_gt_flat = y_seg_test.reshape(-1) > 0.5  # Flatten and threshold ground truths\n",
    "\n",
    "# Metrics calculation\n",
    "# Classification metrics\n",
    "classification_f1 = f1_score(y_class_test, class_preds_labels, average='weighted')\n",
    "classification_cm = confusion_matrix(y_class_test, class_preds_labels)\n",
    "classification_report_txt = classification_report(y_class_test, class_preds_labels)\n",
    "\n",
    "# Segmentation metrics\n",
    "segmentation_f1 = f1_score(seg_gt_flat, seg_preds_flat)\n",
    "segmentation_cm = confusion_matrix(seg_gt_flat, seg_preds_flat)\n",
    "\n",
    "# Printing metrics\n",
    "print(\"Classification F1 Score:\", classification_f1)\n",
    "#print(\"Classification Report:\\n\", classification_report_txt)\n",
    "print(\"Segmentation F1 Score:\", segmentation_f1)\n",
    "\n",
    "# Function to plot confusion matrices with normalization option\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix', f1_score=0.0, decimals=1, normalize=False, small_text=False):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))  # Increased figure size for better clarity\n",
    "    sns.heatmap(cm, annot=True, fmt=f'.{decimals}f' if normalize else 'd', annot_kws={\"size\": 8 if small_text else 10}, linewidths=.5, square=True, cmap='Blues')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(f\"{title} - F1 Score: {f1_score:.2f}\", size=15)\n",
    "\n",
    "# Plotting confusion matrices\n",
    "#plot_confusion_matrix(classification_cm, title='Classification Confusion Matrix (Custom)', f1_score=classification_f1, decimals=1, normalize=True, small_text=True)\n",
    "#plot_confusion_matrix(segmentation_cm, title='Segmentation Confusion Matrix (Custom)', f1_score=segmentation_f1, decimals=2, normalize=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "################# FINE TUNED MODEL ##############\n",
    "\n",
    "# Get predictions from the model\n",
    "class_preds, seg_preds = model_finetuned.predict(test_ds)\n",
    "\n",
    "# Process classification results\n",
    "class_preds_labels = np.argmax(class_preds, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Process segmentation results\n",
    "seg_preds_flat = seg_preds.reshape(-1) > 0.5  # Flatten and threshold predictions\n",
    "seg_gt_flat = y_seg_test.reshape(-1) > 0.5  # Flatten and threshold ground truths\n",
    "\n",
    "# Metrics calculation\n",
    "# Classification metrics\n",
    "classification_f1 = f1_score(y_class_test, class_preds_labels, average='weighted')\n",
    "classification_cm = confusion_matrix(y_class_test, class_preds_labels)\n",
    "classification_report_txt = classification_report(y_class_test, class_preds_labels)\n",
    "\n",
    "# Segmentation metrics\n",
    "segmentation_f1 = f1_score(seg_gt_flat, seg_preds_flat)\n",
    "segmentation_cm = confusion_matrix(seg_gt_flat, seg_preds_flat)\n",
    "\n",
    "# Printing metrics\n",
    "print(\"Classification F1 Score:\", classification_f1)\n",
    "#print(\"Classification Report:\\n\", classification_report_txt)\n",
    "print(\"Segmentation F1 Score:\", segmentation_f1)\n",
    "\n",
    "# Function to plot confusion matrices with normalization option\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix', f1_score=0.0, decimals=1, normalize=False, small_text=False):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))  # Increased figure size for better clarity\n",
    "    sns.heatmap(cm, annot=True, fmt=f'.{decimals}f' if normalize else 'd', annot_kws={\"size\": 8 if small_text else 10}, linewidths=.5, square=True, cmap='Blues')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(f\"{title} - F1 Score: {f1_score:.2f}\", size=15)\n",
    "\n",
    "# Plotting confusion matrices\n",
    "#plot_confusion_matrix(classification_cm, title='Classification Confusion Matrix (Fine-Tuned)', f1_score=classification_f1, decimals=1, normalize=True, small_text=True)\n",
    "#plot_confusion_matrix(segmentation_cm, title='Segmentation Confusion Matrix (Fine-Tuned)', f1_score=segmentation_f1, decimals=2, normalize=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming test_ds is your dataset for testing the model\n",
    "# Assuming model_from_scratch and model_finetuned are your two models\n",
    "\n",
    "# # Model from Scratch\n",
    "# start_time = time.time()\n",
    "# class_preds, seg_preds = model_from_scratch.predict(test_ds)\n",
    "# inference_time_scratch = time.time() - start_time\n",
    "# print(f\"Inference Time for Model from Scratch: {inference_time_scratch:.3f} seconds\")\n",
    "\n",
    "# # Process results and calculate metrics...\n",
    "\n",
    "# # Fine-Tuned Model\n",
    "# start_time = time.time()\n",
    "# class_preds_ft, seg_preds_ft = model_finetuned.predict(test_ds)\n",
    "# inference_time_finetuned = time.time() - start_time\n",
    "# print(f\"Inference Time for Fine-Tuned Model: {inference_time_finetuned:.3f} seconds\")\n",
    "\n",
    "\n",
    "def visualize_segmentation_results(model, dataset, num_images=5):\n",
    "    \"\"\"Visualize segmentation results from the model.\"\"\"\n",
    "    # Get a batch of data\n",
    "    for batch in dataset.take(1):\n",
    "        images, (class_labels, true_masks) = batch\n",
    "    \n",
    "    # Get predictions from the model\n",
    "    class_preds, seg_preds = model.predict(images)\n",
    "    \n",
    "    # Rescale images to [0, 1] for visualization\n",
    "    images = (images * 255.0 + 127.5) / 255.0\n",
    "    seg_preds = seg_preds > 0.5\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, num_images * 5))\n",
    "    for i in range(num_images):\n",
    "        axes[i, 0].imshow(images[i])\n",
    "        axes[i, 0].set_title(\"Input Image\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(true_masks[i].numpy().squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title(\"True Mask\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(seg_preds[i].squeeze(), cmap='gray')\n",
    "        axes[i, 2].set_title(\"Predicted Mask\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results from the model trained from scratch\n",
    "visualize_segmentation_results(model_from_scratch, test_ds)\n",
    "\n",
    "# Visualize results from the fine-tuned model\n",
    "#visualize_segmentation_results(model_finetuned, test_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf92b4f-3057-4b0f-ade3-5700ea912214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your write-up, you should include:\n",
    "# - A discussion of what pre-processing (i.e. resizing, colour conversion, augmentation, etc.) you apply to the data and why.\n",
    "# - Details of two implemented methods. This should include a details of the final “from-scratch” approach and justification \n",
    "#   for the chosen design, and details of changes made to MobileNetV3Small for the “fine-tuned” approach. Details on how the \n",
    "#   models are trained are also to be provided. \n",
    "# - An evaluation that compares the two models for the two tasks (classification and semantic segmentation). Your evaluation \n",
    "#   should discuss overall model performance, how it differs between the two approaches, and include figures if/where necessary.\n",
    "# - A discussion of methods that were explored to improve performance for both models and mitigate identified issues, and potentially \n",
    "#   other methods that were considered but not implemented due to computational constraints. See the assignment brief for further \n",
    "#   details.\n",
    "# Your write-up should be supported by appropriate figures and tables. Figures and tables should have numbers and meaningful captions\n",
    "#\n",
    "# SEE THE ASSIGNMENT BRIEF ON CANVAS FOR MORE DETAILS AND NOTE THAT A NOTEBOOK FILE DOES NOT CONSTITUTE A VALID SUBMISSION. \n",
    "# YOU SHOULD WRITE UP YOUR RESPONSE IN A SEPARATE DOCUMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
